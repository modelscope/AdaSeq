experiment:
  exp_dir: experiments/
  exp_name: ufet
  seed: 42

task: entity-typing

dataset:
  data_file:
    train: 'https://www.modelscope.cn/api/v1/datasets/izhx404/ufet/repo/files?Revision=master&FilePath=train.json'
    valid: 'https://www.modelscope.cn/api/v1/datasets/izhx404/ufet/repo/files?Revision=master&FilePath=dev.json'
    test: 'https://www.modelscope.cn/api/v1/datasets/izhx404/ufet/repo/files?Revision=master&FilePath=test.json'
  tokenizer: blank
  lower: true
  labels: 'https://www.modelscope.cn/api/v1/datasets/izhx404/ufet/repo/files?Revision=master&FilePath=labels.txt'

preprocessor:
  type: multilabel-concat-typing-preprocessor
  model_dir:  roberta-large
  max_length: 150

data_collator: MultiLabelConcatTypingDataCollatorWithPadding

model:
  type: multilabel-concat-typing-model
  encoder:
    model_name_or_path: roberta-large
    drop_special_tokens: false
  word_dropout: 0
  decoder:
    type: linear
  loss_function: WBCE
  pos_weight: 4

train:
  max_epochs: 100
  dataloader:
    batch_size_per_gpu: 8
  optimizer:
    type: AdamW
    lr: 5.0e-5
  lr_scheduler:
    type: cosine
    warmup_rate: 0.1 # when choose concat typing model, default to use cosine_linear_with_warmup
    options:
      by_epoch: false
  hooks:
    - type: "CheckpointHook"
      interval: 100
    - type: "BestCkptSaverHook"
      save_file_name: "best_model.pt"

evaluation:
  dataloader:
    batch_size_per_gpu: 32
  metrics: typing-metric
