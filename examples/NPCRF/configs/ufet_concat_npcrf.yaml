experiment:
  exp_dir: experiments/
  exp_name: ufet
  seed: 42

task: entity-typing

dataset:
  data_file:
    train: 'https://www.modelscope.cn/api/v1/datasets/izhx404/ufet/repo/files?Revision=master&FilePath=train.json'
    valid: 'https://www.modelscope.cn/api/v1/datasets/izhx404/ufet/repo/files?Revision=master&FilePath=dev.json'
    test: 'https://www.modelscope.cn/api/v1/datasets/izhx404/ufet/repo/files?Revision=master&FilePath=test.json'
  tokenizer: blank
  lower: true
  labels: 'https://www.modelscope.cn/api/v1/datasets/izhx404/ufet/repo/files?Revision=master&FilePath=labels.txt'

preprocessor:
  type: multilabel-concat-typing-preprocessor
  model_dir:  roberta-large
  max_length: 150

data_collator: MultiLabelConcatTypingDataCollatorWithPadding

model:
  type: multilabel-concat-typing-model
  embedder:
    model_name_or_path: roberta-large
    drop_special_tokens: false
  word_dropout: 0
  decoder:
    type: pairwise-crf
    label_emb_type: glove
    label_emb_dim: 300
    source_emb_file_path: ${PATH_TO_DIR}/glove.6B.300d.txt
    target_emb_dir: ${PATH_TO_DIR}  # TODO
    target_emb_name: glove.300.emb
    pairwise_factor: 70
    mfvi_iteration: 6
    two_potential: false
    sign_trick: true
  loss_function: WBCE-UFET
  pos_weight: 4

train:
  max_epochs: 30
  dataloader:
    batch_size_per_gpu: 4
  optimizer:
    type: AdamW
    lr: 2.0e-5
  lr_scheduler:
    type: cosine
    warmup_rate: 0.1 # when choose concat typing model, default to use cosine_linear_with_warmup
    options:
      by_epoch: false
  hooks:
    - type: "CheckpointHook"
      interval: 100
    - type: "BestCkptSaverHook"
      save_file_name: "best_model.pt"

evaluation:
  dataloader:
    batch_size_per_gpu: 32
  metrics: typing-metric
